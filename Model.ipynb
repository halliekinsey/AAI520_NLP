{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dialogues: 83098\n",
      "\n",
      "First dialogue: <genre:comedy,romance><char:BIANCA><char:CAMERON>\n",
      "BIANCA: Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
      "CAMERON: Well, I thought we'd start with pronunciation, if that's okay with you.\n",
      "BIANCA: Not the hacking and gagging and spitting part.  Please.\n",
      "CAMERON: Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
      "\n",
      "Second dialogue: \n",
      "<genre:comedy,romance><char:BIANCA><char:CAMERON>\n",
      "BIANCA: You're asking me out.  That's so cute. What's your name again?\n",
      "CAMERON: Forget it.\n",
      "\n",
      "Third dialogue: \n",
      "<genre:comedy,romance><char:BIANCA><char:CAMERON>\n",
      "BIANCA: No, no, it's my fault -- we didn't have a proper introduction ---\n",
      "CAMERON: Cameron.\n",
      "BIANCA: The thing is, Cameron -- I'm at the mercy of a particularly hideous breed of loser.  My sister.  I can't date until she does.\n",
      "CAMERON: Seems like she could get a date easy enough...\n"
     ]
    }
   ],
   "source": [
    "data = open(\"processed_dialogs.txt\", \"r\").read()\n",
    "dialogues = data.split(\"\\n\\n\")\n",
    "print(f\"Number of dialogues: {len(dialogues)}\")\n",
    "print(f\"\\nFirst dialogue: {dialogues[0]}\")\n",
    "print(f\"\\nSecond dialogue: {dialogues[1]}\")\n",
    "print(f\"\\nThird dialogue: {dialogues[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model V1  \n",
    "This version split up the entire input text into blocks. It did this arbitrarily rather than with respect to the different conversation threads which often resulted in the model choosing to start an entirely new dialog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec552a5425a4437a82842ab97a1efd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4152, 'grad_norm': 3.140174388885498, 'learning_rate': 4.8419022323404794e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3712, 'grad_norm': 2.992849826812744, 'learning_rate': 4.6838044646809586e-05, 'epoch': 0.06}\n",
      "{'loss': 2.3226, 'grad_norm': 2.2799367904663086, 'learning_rate': 4.5257066970214385e-05, 'epoch': 0.09}\n",
      "{'loss': 2.3184, 'grad_norm': 2.223583459854126, 'learning_rate': 4.367608929361918e-05, 'epoch': 0.13}\n",
      "{'loss': 2.266, 'grad_norm': 2.1571054458618164, 'learning_rate': 4.209511161702397e-05, 'epoch': 0.16}\n",
      "{'loss': 2.2801, 'grad_norm': 2.1511244773864746, 'learning_rate': 4.051413394042876e-05, 'epoch': 0.19}\n",
      "{'loss': 2.2761, 'grad_norm': 2.174072504043579, 'learning_rate': 3.893315626383356e-05, 'epoch': 0.22}\n",
      "{'loss': 2.2812, 'grad_norm': 1.7256395816802979, 'learning_rate': 3.735217858723835e-05, 'epoch': 0.25}\n",
      "{'loss': 2.2461, 'grad_norm': 2.0350048542022705, 'learning_rate': 3.5771200910643144e-05, 'epoch': 0.28}\n",
      "{'loss': 2.2678, 'grad_norm': 2.028801918029785, 'learning_rate': 3.4190223234047936e-05, 'epoch': 0.32}\n",
      "{'loss': 2.2384, 'grad_norm': 2.0904128551483154, 'learning_rate': 3.260924555745273e-05, 'epoch': 0.35}\n",
      "{'loss': 2.2367, 'grad_norm': 1.8867262601852417, 'learning_rate': 3.1028267880857526e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2584, 'grad_norm': 2.0414886474609375, 'learning_rate': 2.9447290204262318e-05, 'epoch': 0.41}\n",
      "{'loss': 2.225, 'grad_norm': 1.911708950996399, 'learning_rate': 2.786631252766711e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2283, 'grad_norm': 1.9679248332977295, 'learning_rate': 2.6285334851071902e-05, 'epoch': 0.47}\n",
      "{'loss': 2.2326, 'grad_norm': 1.8361082077026367, 'learning_rate': 2.4704357174476697e-05, 'epoch': 0.51}\n",
      "{'loss': 2.2506, 'grad_norm': 2.192887306213379, 'learning_rate': 2.3123379497881493e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2126, 'grad_norm': 1.9174914360046387, 'learning_rate': 2.1542401821286285e-05, 'epoch': 0.57}\n",
      "{'loss': 2.203, 'grad_norm': 1.5193051099777222, 'learning_rate': 1.9961424144691077e-05, 'epoch': 0.6}\n",
      "{'loss': 2.218, 'grad_norm': 2.1238210201263428, 'learning_rate': 1.8380446468095872e-05, 'epoch': 0.63}\n",
      "{'loss': 2.1838, 'grad_norm': 1.9703052043914795, 'learning_rate': 1.6799468791500664e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2175, 'grad_norm': 1.960647702217102, 'learning_rate': 1.521849111490546e-05, 'epoch': 0.7}\n",
      "{'loss': 2.1905, 'grad_norm': 2.0092661380767822, 'learning_rate': 1.3637513438310251e-05, 'epoch': 0.73}\n",
      "{'loss': 2.1843, 'grad_norm': 2.1512527465820312, 'learning_rate': 1.2056535761715045e-05, 'epoch': 0.76}\n",
      "{'loss': 2.1839, 'grad_norm': 1.8570852279663086, 'learning_rate': 1.0475558085119839e-05, 'epoch': 0.79}\n",
      "{'loss': 2.1915, 'grad_norm': 2.418527603149414, 'learning_rate': 8.894580408524632e-06, 'epoch': 0.82}\n",
      "{'loss': 2.1753, 'grad_norm': 1.9386308193206787, 'learning_rate': 7.313602731929425e-06, 'epoch': 0.85}\n",
      "{'loss': 2.1887, 'grad_norm': 2.206268310546875, 'learning_rate': 5.732625055334219e-06, 'epoch': 0.89}\n",
      "{'loss': 2.1758, 'grad_norm': 1.8272827863693237, 'learning_rate': 4.1516473787390124e-06, 'epoch': 0.92}\n",
      "{'loss': 2.216, 'grad_norm': 2.017228364944458, 'learning_rate': 2.5706697021438057e-06, 'epoch': 0.95}\n",
      "{'loss': 2.1991, 'grad_norm': 1.885785460472107, 'learning_rate': 9.896920255485993e-07, 'epoch': 0.98}\n",
      "{'train_runtime': 1354.2279, 'train_samples_per_second': 46.707, 'train_steps_per_second': 11.677, 'train_loss': 2.23960803610953, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-finetuned\\\\tokenizer_config.json',\n",
       " './gpt2-finetuned\\\\special_tokens_map.json',\n",
       " './gpt2-finetuned\\\\vocab.json',\n",
       " './gpt2-finetuned\\\\merges.txt',\n",
       " './gpt2-finetuned\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Prepare your dataset\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"processed_dialogs.txt\",\n",
    "    block_size=128)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./gpt2-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<genre:comedy,romance><char:BIANCA><char:CAMERON>\n",
      "BIANCA: They're having a fight... again.\n",
      "CAMPBELL: I'm sorry, I didn't mean to...\n",
      "BANDA: You're not going to be able to get out of here.\n",
      "\n",
      "\n",
      "<char:\"BIanCA\"><char\":CEMETER>\n",
      "\n",
      "<family:biography,drama,history><family:\"BENNY><b:BUD><d:DOUG>BUNNY: What's the matter?\n",
      "DUDG: Nothing.  I just want to talk to you. I want you to know that I love you, and I don't want anyone to see that. You know, you're a good man. And I know you love me. But I can't let you go. It's not right. We're going back to the hotel. The hotel is closed\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetuned\")\n",
    "\n",
    "# Prepare input\n",
    "input_text = \"<genre:comedy,romance><char:BIANCA><char:CAMERON>\\nBIANCA: They're having a fight... again.\"\n",
    "\n",
    "# Encode input\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate output\n",
    "output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decode and print the result\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model V2  \n",
    "This model is trained on samples of entire conversations which resulted in an improved ability to stay in one genre and between the same two characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf16b4105d34026848784615dc262f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20775 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3665, 'grad_norm': 4.240726470947266, 'learning_rate': 4.879663056558363e-05, 'epoch': 0.02}\n",
      "{'loss': 2.2709, 'grad_norm': 2.9654688835144043, 'learning_rate': 4.759326113116727e-05, 'epoch': 0.05}\n",
      "{'loss': 2.2172, 'grad_norm': 3.0672502517700195, 'learning_rate': 4.63898916967509e-05, 'epoch': 0.07}\n",
      "{'loss': 2.2186, 'grad_norm': 3.00846004486084, 'learning_rate': 4.518652226233454e-05, 'epoch': 0.1}\n",
      "{'loss': 2.2369, 'grad_norm': 3.286755323410034, 'learning_rate': 4.398315282791817e-05, 'epoch': 0.12}\n",
      "{'loss': 2.2261, 'grad_norm': 2.3606505393981934, 'learning_rate': 4.277978339350181e-05, 'epoch': 0.14}\n",
      "{'loss': 2.1868, 'grad_norm': 3.0841352939605713, 'learning_rate': 4.1576413959085445e-05, 'epoch': 0.17}\n",
      "{'loss': 2.1673, 'grad_norm': 2.4785640239715576, 'learning_rate': 4.0373044524669075e-05, 'epoch': 0.19}\n",
      "{'loss': 2.1476, 'grad_norm': 2.0501327514648438, 'learning_rate': 3.916967509025271e-05, 'epoch': 0.22}\n",
      "{'loss': 2.1452, 'grad_norm': 2.8682167530059814, 'learning_rate': 3.7966305655836343e-05, 'epoch': 0.24}\n",
      "{'loss': 2.1499, 'grad_norm': 2.4254660606384277, 'learning_rate': 3.676293622141998e-05, 'epoch': 0.26}\n",
      "{'loss': 2.1325, 'grad_norm': 2.516324758529663, 'learning_rate': 3.555956678700361e-05, 'epoch': 0.29}\n",
      "{'loss': 2.1318, 'grad_norm': 2.9864113330841064, 'learning_rate': 3.435619735258724e-05, 'epoch': 0.31}\n",
      "{'loss': 2.106, 'grad_norm': 2.7039968967437744, 'learning_rate': 3.315282791817088e-05, 'epoch': 0.34}\n",
      "{'loss': 2.1483, 'grad_norm': 2.3638644218444824, 'learning_rate': 3.194945848375451e-05, 'epoch': 0.36}\n",
      "{'loss': 2.0995, 'grad_norm': 2.334022283554077, 'learning_rate': 3.074608904933815e-05, 'epoch': 0.39}\n",
      "{'loss': 2.089, 'grad_norm': 2.675896406173706, 'learning_rate': 2.9542719614921782e-05, 'epoch': 0.41}\n",
      "{'loss': 2.1032, 'grad_norm': 2.544827938079834, 'learning_rate': 2.8339350180505413e-05, 'epoch': 0.43}\n",
      "{'loss': 2.0655, 'grad_norm': 2.284426689147949, 'learning_rate': 2.713598074608905e-05, 'epoch': 0.46}\n",
      "{'loss': 2.07, 'grad_norm': 2.134601593017578, 'learning_rate': 2.5932611311672685e-05, 'epoch': 0.48}\n",
      "{'loss': 2.0614, 'grad_norm': 2.6997532844543457, 'learning_rate': 2.472924187725632e-05, 'epoch': 0.51}\n",
      "{'loss': 2.0753, 'grad_norm': 2.2453227043151855, 'learning_rate': 2.3525872442839953e-05, 'epoch': 0.53}\n",
      "{'loss': 2.0742, 'grad_norm': 2.2066268920898438, 'learning_rate': 2.2322503008423587e-05, 'epoch': 0.55}\n",
      "{'loss': 2.0416, 'grad_norm': 1.9847118854522705, 'learning_rate': 2.111913357400722e-05, 'epoch': 0.58}\n",
      "{'loss': 2.0586, 'grad_norm': 2.6713151931762695, 'learning_rate': 1.9915764139590855e-05, 'epoch': 0.6}\n",
      "{'loss': 2.0531, 'grad_norm': 2.2925753593444824, 'learning_rate': 1.871239470517449e-05, 'epoch': 0.63}\n",
      "{'loss': 2.0319, 'grad_norm': 1.865940809249878, 'learning_rate': 1.7509025270758123e-05, 'epoch': 0.65}\n",
      "{'loss': 2.0443, 'grad_norm': 2.0766971111297607, 'learning_rate': 1.6305655836341757e-05, 'epoch': 0.67}\n",
      "{'loss': 2.0162, 'grad_norm': 3.1454668045043945, 'learning_rate': 1.5102286401925391e-05, 'epoch': 0.7}\n",
      "{'loss': 2.045, 'grad_norm': 2.6898529529571533, 'learning_rate': 1.3898916967509026e-05, 'epoch': 0.72}\n",
      "{'loss': 2.0445, 'grad_norm': 2.086174726486206, 'learning_rate': 1.2695547533092661e-05, 'epoch': 0.75}\n",
      "{'loss': 2.0194, 'grad_norm': 2.18049955368042, 'learning_rate': 1.1492178098676294e-05, 'epoch': 0.77}\n",
      "{'loss': 2.0324, 'grad_norm': 2.4252371788024902, 'learning_rate': 1.028880866425993e-05, 'epoch': 0.79}\n",
      "{'loss': 2.0323, 'grad_norm': 2.341942071914673, 'learning_rate': 9.085439229843562e-06, 'epoch': 0.82}\n",
      "{'loss': 2.0442, 'grad_norm': 2.3043715953826904, 'learning_rate': 7.882069795427196e-06, 'epoch': 0.84}\n",
      "{'loss': 2.0448, 'grad_norm': 2.684467315673828, 'learning_rate': 6.678700361010831e-06, 'epoch': 0.87}\n",
      "{'loss': 2.0302, 'grad_norm': 2.314789056777954, 'learning_rate': 5.475330926594465e-06, 'epoch': 0.89}\n",
      "{'loss': 2.0343, 'grad_norm': 2.1284968852996826, 'learning_rate': 4.271961492178099e-06, 'epoch': 0.91}\n",
      "{'loss': 2.0293, 'grad_norm': 2.4306623935699463, 'learning_rate': 3.068592057761733e-06, 'epoch': 0.94}\n",
      "{'loss': 2.0188, 'grad_norm': 2.174391508102417, 'learning_rate': 1.865222623345367e-06, 'epoch': 0.96}\n",
      "{'loss': 2.0102, 'grad_norm': 2.68711519241333, 'learning_rate': 6.618531889290013e-07, 'epoch': 0.99}\n",
      "{'train_runtime': 1919.5813, 'train_samples_per_second': 43.29, 'train_steps_per_second': 10.823, 'train_loss': 2.0993883645692315, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-finetunedV2\\\\tokenizer_config.json',\n",
       " './gpt2-finetunedV2\\\\special_tokens_map.json',\n",
       " './gpt2-finetunedV2\\\\vocab.json',\n",
       " './gpt2-finetunedV2\\\\merges.txt',\n",
       " './gpt2-finetunedV2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        with open(file_path, \"r\") as f:\n",
    "            self.dialogs = f.read().split(\"\\n\\n\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dialogs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dialog = self.dialogs[idx]\n",
    "        encodings = self.tokenizer(dialog, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        return {key: torch.squeeze(val) for key, val in encodings.items()}\n",
    "    \n",
    "# Prepare your dataset\n",
    "train_dataset = DialogDataset(\n",
    "    file_path=\"processed_dialogs.txt\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetunedV2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./gpt2-finetunedV2\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetunedV2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<genre:comedy><char:BIANCA><char:CAMERON>\n",
      "BIANCA: They're having a fight... again.\n",
      "CAMS: I'm sorry, I don't know.  I just... I...\n",
      "BAN CA: You're not going to let me go. I'll be back in a minute. You can't go back. It's not my fault. We're going back to the hotel. And I want you to know that. Because I know you're right. But I can tell you that I've been very careful. That I have been careful for a long time. So I think I should tell the truth. The truth is, you know, it's a very difficult thing to do. Especially when you have to go to a hospital. There's no way you can go home without knowing. If you donï¿½t know what youïve done, then you'll never get to see your family\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetunedV2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetunedV2\")\n",
    "\n",
    "# Prepare input\n",
    "input_text = \"<genre:comedy><char:BIANCA><char:CAMERON>\\nBIANCA: They're having a fight... again.\"\n",
    "\n",
    "# Encode input\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate output\n",
    "output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decode and print the result\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model V2 - 3 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86724df98f384e809c50866c5997fc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3664, 'grad_norm': 4.189531326293945, 'learning_rate': 4.959887685519455e-05, 'epoch': 0.02}\n",
      "{'loss': 2.2713, 'grad_norm': 2.9324004650115967, 'learning_rate': 4.9197753710389094e-05, 'epoch': 0.05}\n",
      "{'loss': 2.2188, 'grad_norm': 3.052821159362793, 'learning_rate': 4.879663056558363e-05, 'epoch': 0.07}\n",
      "{'loss': 2.2199, 'grad_norm': 2.948228597640991, 'learning_rate': 4.839550742077818e-05, 'epoch': 0.1}\n",
      "{'loss': 2.2387, 'grad_norm': 3.591900110244751, 'learning_rate': 4.7994384275972725e-05, 'epoch': 0.12}\n",
      "{'loss': 2.2278, 'grad_norm': 2.323040246963501, 'learning_rate': 4.759326113116727e-05, 'epoch': 0.14}\n",
      "{'loss': 2.1895, 'grad_norm': 3.0196328163146973, 'learning_rate': 4.7192137986361816e-05, 'epoch': 0.17}\n",
      "{'loss': 2.1706, 'grad_norm': 2.442878484725952, 'learning_rate': 4.6791014841556355e-05, 'epoch': 0.19}\n",
      "{'loss': 2.1507, 'grad_norm': 1.990504264831543, 'learning_rate': 4.63898916967509e-05, 'epoch': 0.22}\n",
      "{'loss': 2.1482, 'grad_norm': 2.855936050415039, 'learning_rate': 4.598876855194545e-05, 'epoch': 0.24}\n",
      "{'loss': 2.1528, 'grad_norm': 2.2785706520080566, 'learning_rate': 4.558764540713999e-05, 'epoch': 0.26}\n",
      "{'loss': 2.1351, 'grad_norm': 2.4638025760650635, 'learning_rate': 4.518652226233454e-05, 'epoch': 0.29}\n",
      "{'loss': 2.1343, 'grad_norm': 2.853442668914795, 'learning_rate': 4.478539911752908e-05, 'epoch': 0.31}\n",
      "{'loss': 2.1083, 'grad_norm': 2.5983026027679443, 'learning_rate': 4.4384275972723624e-05, 'epoch': 0.34}\n",
      "{'loss': 2.1506, 'grad_norm': 2.2574121952056885, 'learning_rate': 4.398315282791817e-05, 'epoch': 0.36}\n",
      "{'loss': 2.1015, 'grad_norm': 2.2202956676483154, 'learning_rate': 4.3582029683112715e-05, 'epoch': 0.39}\n",
      "{'loss': 2.0908, 'grad_norm': 2.538898468017578, 'learning_rate': 4.318090653830726e-05, 'epoch': 0.41}\n",
      "{'loss': 2.1049, 'grad_norm': 2.396144390106201, 'learning_rate': 4.277978339350181e-05, 'epoch': 0.43}\n",
      "{'loss': 2.0668, 'grad_norm': 2.1366138458251953, 'learning_rate': 4.237866024869635e-05, 'epoch': 0.46}\n",
      "{'loss': 2.0715, 'grad_norm': 1.9840930700302124, 'learning_rate': 4.19775371038909e-05, 'epoch': 0.48}\n",
      "{'loss': 2.0626, 'grad_norm': 2.468315362930298, 'learning_rate': 4.1576413959085445e-05, 'epoch': 0.51}\n",
      "{'loss': 2.075, 'grad_norm': 2.043100595474243, 'learning_rate': 4.117529081427999e-05, 'epoch': 0.53}\n",
      "{'loss': 2.0746, 'grad_norm': 2.0409326553344727, 'learning_rate': 4.0774167669474536e-05, 'epoch': 0.55}\n",
      "{'loss': 2.0415, 'grad_norm': 1.8235228061676025, 'learning_rate': 4.0373044524669075e-05, 'epoch': 0.58}\n",
      "{'loss': 2.058, 'grad_norm': 2.488405704498291, 'learning_rate': 3.997192137986362e-05, 'epoch': 0.6}\n",
      "{'loss': 2.0523, 'grad_norm': 2.132418394088745, 'learning_rate': 3.957079823505817e-05, 'epoch': 0.63}\n",
      "{'loss': 2.0295, 'grad_norm': 1.726885199546814, 'learning_rate': 3.916967509025271e-05, 'epoch': 0.65}\n",
      "{'loss': 2.0428, 'grad_norm': 1.9037384986877441, 'learning_rate': 3.876855194544726e-05, 'epoch': 0.67}\n",
      "{'loss': 2.014, 'grad_norm': 2.738478899002075, 'learning_rate': 3.83674288006418e-05, 'epoch': 0.7}\n",
      "{'loss': 2.043, 'grad_norm': 2.4652419090270996, 'learning_rate': 3.7966305655836343e-05, 'epoch': 0.72}\n",
      "{'loss': 2.0418, 'grad_norm': 1.863530158996582, 'learning_rate': 3.756518251103089e-05, 'epoch': 0.75}\n",
      "{'loss': 2.0151, 'grad_norm': 1.8262279033660889, 'learning_rate': 3.7164059366225435e-05, 'epoch': 0.77}\n",
      "{'loss': 2.0283, 'grad_norm': 2.1691691875457764, 'learning_rate': 3.676293622141998e-05, 'epoch': 0.79}\n",
      "{'loss': 2.0274, 'grad_norm': 2.0588746070861816, 'learning_rate': 3.636181307661452e-05, 'epoch': 0.82}\n",
      "{'loss': 2.0386, 'grad_norm': 1.9992061853408813, 'learning_rate': 3.5960689931809066e-05, 'epoch': 0.84}\n",
      "{'loss': 2.0378, 'grad_norm': 2.9637575149536133, 'learning_rate': 3.555956678700361e-05, 'epoch': 0.87}\n",
      "{'loss': 2.0213, 'grad_norm': 2.042381763458252, 'learning_rate': 3.515844364219816e-05, 'epoch': 0.89}\n",
      "{'loss': 2.0258, 'grad_norm': 1.9771448373794556, 'learning_rate': 3.47573204973927e-05, 'epoch': 0.91}\n",
      "{'loss': 2.0175, 'grad_norm': 2.219280958175659, 'learning_rate': 3.435619735258724e-05, 'epoch': 0.94}\n",
      "{'loss': 2.0065, 'grad_norm': 2.125303030014038, 'learning_rate': 3.395507420778179e-05, 'epoch': 0.96}\n",
      "{'loss': 1.9959, 'grad_norm': 2.3337771892547607, 'learning_rate': 3.3553951062976334e-05, 'epoch': 0.99}\n",
      "{'loss': 1.9757, 'grad_norm': 2.3443100452423096, 'learning_rate': 3.315282791817088e-05, 'epoch': 1.01}\n",
      "{'loss': 1.9236, 'grad_norm': 1.7995961904525757, 'learning_rate': 3.2751704773365426e-05, 'epoch': 1.03}\n",
      "{'loss': 1.9174, 'grad_norm': 1.884700894355774, 'learning_rate': 3.2350581628559965e-05, 'epoch': 1.06}\n",
      "{'loss': 1.8941, 'grad_norm': 2.372041940689087, 'learning_rate': 3.194945848375451e-05, 'epoch': 1.08}\n",
      "{'loss': 1.9119, 'grad_norm': 2.355761766433716, 'learning_rate': 3.1548335338949056e-05, 'epoch': 1.11}\n",
      "{'loss': 1.9012, 'grad_norm': 2.0313162803649902, 'learning_rate': 3.11472121941436e-05, 'epoch': 1.13}\n",
      "{'loss': 1.9045, 'grad_norm': 2.390505790710449, 'learning_rate': 3.074608904933815e-05, 'epoch': 1.16}\n",
      "{'loss': 1.9189, 'grad_norm': 2.018430471420288, 'learning_rate': 3.034496590453269e-05, 'epoch': 1.18}\n",
      "{'loss': 1.8845, 'grad_norm': 2.144022226333618, 'learning_rate': 2.9943842759727236e-05, 'epoch': 1.2}\n",
      "{'loss': 1.9298, 'grad_norm': 2.834010362625122, 'learning_rate': 2.9542719614921782e-05, 'epoch': 1.23}\n",
      "{'loss': 1.8797, 'grad_norm': 2.1546740531921387, 'learning_rate': 2.9141596470116328e-05, 'epoch': 1.25}\n",
      "{'loss': 1.8914, 'grad_norm': 2.0118486881256104, 'learning_rate': 2.8740473325310874e-05, 'epoch': 1.28}\n",
      "{'loss': 1.9124, 'grad_norm': 2.3938348293304443, 'learning_rate': 2.8339350180505413e-05, 'epoch': 1.3}\n",
      "{'loss': 1.9053, 'grad_norm': 2.291208267211914, 'learning_rate': 2.793822703569996e-05, 'epoch': 1.32}\n",
      "{'loss': 1.9102, 'grad_norm': 2.3712549209594727, 'learning_rate': 2.7537103890894505e-05, 'epoch': 1.35}\n",
      "{'loss': 1.9112, 'grad_norm': 2.211184024810791, 'learning_rate': 2.713598074608905e-05, 'epoch': 1.37}\n",
      "{'loss': 1.9111, 'grad_norm': 1.7150012254714966, 'learning_rate': 2.6734857601283596e-05, 'epoch': 1.4}\n",
      "{'loss': 1.8978, 'grad_norm': 2.296257495880127, 'learning_rate': 2.633373445647814e-05, 'epoch': 1.42}\n",
      "{'loss': 1.9046, 'grad_norm': 2.3184807300567627, 'learning_rate': 2.5932611311672685e-05, 'epoch': 1.44}\n",
      "{'loss': 1.8954, 'grad_norm': 2.165598154067993, 'learning_rate': 2.553148816686723e-05, 'epoch': 1.47}\n",
      "{'loss': 1.91, 'grad_norm': 2.259575128555298, 'learning_rate': 2.5130365022061776e-05, 'epoch': 1.49}\n",
      "{'loss': 1.8891, 'grad_norm': 2.26118540763855, 'learning_rate': 2.472924187725632e-05, 'epoch': 1.52}\n",
      "{'loss': 1.8944, 'grad_norm': 2.141113042831421, 'learning_rate': 2.4328118732450864e-05, 'epoch': 1.54}\n",
      "{'loss': 1.8791, 'grad_norm': 1.9095646142959595, 'learning_rate': 2.392699558764541e-05, 'epoch': 1.56}\n",
      "{'loss': 1.9163, 'grad_norm': 1.8949590921401978, 'learning_rate': 2.3525872442839953e-05, 'epoch': 1.59}\n",
      "{'loss': 1.8816, 'grad_norm': 2.175194263458252, 'learning_rate': 2.31247492980345e-05, 'epoch': 1.61}\n",
      "{'loss': 1.8763, 'grad_norm': 2.048931121826172, 'learning_rate': 2.272362615322904e-05, 'epoch': 1.64}\n",
      "{'loss': 1.8703, 'grad_norm': 2.2548775672912598, 'learning_rate': 2.2322503008423587e-05, 'epoch': 1.66}\n",
      "{'loss': 1.9105, 'grad_norm': 1.8985825777053833, 'learning_rate': 2.1921379863618133e-05, 'epoch': 1.68}\n",
      "{'loss': 1.8803, 'grad_norm': 2.5509870052337646, 'learning_rate': 2.1520256718812675e-05, 'epoch': 1.71}\n",
      "{'loss': 1.8693, 'grad_norm': 2.209230422973633, 'learning_rate': 2.111913357400722e-05, 'epoch': 1.73}\n",
      "{'loss': 1.8812, 'grad_norm': 2.0820140838623047, 'learning_rate': 2.0718010429201763e-05, 'epoch': 1.76}\n",
      "{'loss': 1.8959, 'grad_norm': 2.494345188140869, 'learning_rate': 2.031688728439631e-05, 'epoch': 1.78}\n",
      "{'loss': 1.8991, 'grad_norm': 2.3277838230133057, 'learning_rate': 1.9915764139590855e-05, 'epoch': 1.81}\n",
      "{'loss': 1.8792, 'grad_norm': 2.1281750202178955, 'learning_rate': 1.95146409947854e-05, 'epoch': 1.83}\n",
      "{'loss': 1.8703, 'grad_norm': 2.2267158031463623, 'learning_rate': 1.9113517849979943e-05, 'epoch': 1.85}\n",
      "{'loss': 1.8774, 'grad_norm': 2.1129322052001953, 'learning_rate': 1.871239470517449e-05, 'epoch': 1.88}\n",
      "{'loss': 1.9053, 'grad_norm': 2.094599485397339, 'learning_rate': 1.8311271560369035e-05, 'epoch': 1.9}\n",
      "{'loss': 1.8687, 'grad_norm': 1.8859246969223022, 'learning_rate': 1.791014841556358e-05, 'epoch': 1.93}\n",
      "{'loss': 1.902, 'grad_norm': 2.0589468479156494, 'learning_rate': 1.7509025270758123e-05, 'epoch': 1.95}\n",
      "{'loss': 1.8668, 'grad_norm': 1.9428439140319824, 'learning_rate': 1.710790212595267e-05, 'epoch': 1.97}\n",
      "{'loss': 1.8979, 'grad_norm': 1.9715776443481445, 'learning_rate': 1.6706778981147215e-05, 'epoch': 2.0}\n",
      "{'loss': 1.8126, 'grad_norm': 2.4670255184173584, 'learning_rate': 1.6305655836341757e-05, 'epoch': 2.02}\n",
      "{'loss': 1.8127, 'grad_norm': 2.019927501678467, 'learning_rate': 1.5904532691536303e-05, 'epoch': 2.05}\n",
      "{'loss': 1.8151, 'grad_norm': 2.5346620082855225, 'learning_rate': 1.5503409546730846e-05, 'epoch': 2.07}\n",
      "{'loss': 1.8001, 'grad_norm': 2.603405475616455, 'learning_rate': 1.5102286401925391e-05, 'epoch': 2.09}\n",
      "{'loss': 1.8107, 'grad_norm': 2.1984970569610596, 'learning_rate': 1.4701163257119937e-05, 'epoch': 2.12}\n",
      "{'loss': 1.8076, 'grad_norm': 2.279656171798706, 'learning_rate': 1.4300040112314481e-05, 'epoch': 2.14}\n",
      "{'loss': 1.8039, 'grad_norm': 2.0423521995544434, 'learning_rate': 1.3898916967509026e-05, 'epoch': 2.17}\n",
      "{'loss': 1.828, 'grad_norm': 2.2586092948913574, 'learning_rate': 1.349779382270357e-05, 'epoch': 2.19}\n",
      "{'loss': 1.8301, 'grad_norm': 2.4169344902038574, 'learning_rate': 1.3096670677898116e-05, 'epoch': 2.21}\n",
      "{'loss': 1.8137, 'grad_norm': 2.547712564468384, 'learning_rate': 1.2695547533092661e-05, 'epoch': 2.24}\n",
      "{'loss': 1.8274, 'grad_norm': 2.255485773086548, 'learning_rate': 1.2294424388287206e-05, 'epoch': 2.26}\n",
      "{'loss': 1.8021, 'grad_norm': 2.6168782711029053, 'learning_rate': 1.189330124348175e-05, 'epoch': 2.29}\n",
      "{'loss': 1.8067, 'grad_norm': 2.3675684928894043, 'learning_rate': 1.1492178098676294e-05, 'epoch': 2.31}\n",
      "{'loss': 1.7993, 'grad_norm': 2.115946054458618, 'learning_rate': 1.1091054953870838e-05, 'epoch': 2.33}\n",
      "{'loss': 1.7815, 'grad_norm': 2.1644222736358643, 'learning_rate': 1.0689931809065384e-05, 'epoch': 2.36}\n",
      "{'loss': 1.8065, 'grad_norm': 2.2566041946411133, 'learning_rate': 1.028880866425993e-05, 'epoch': 2.38}\n",
      "{'loss': 1.8002, 'grad_norm': 2.3200955390930176, 'learning_rate': 9.887685519454474e-06, 'epoch': 2.41}\n",
      "{'loss': 1.8216, 'grad_norm': 1.8993349075317383, 'learning_rate': 9.486562374649018e-06, 'epoch': 2.43}\n",
      "{'loss': 1.7761, 'grad_norm': 2.3386616706848145, 'learning_rate': 9.085439229843562e-06, 'epoch': 2.45}\n",
      "{'loss': 1.8277, 'grad_norm': 2.071303606033325, 'learning_rate': 8.684316085038108e-06, 'epoch': 2.48}\n",
      "{'loss': 1.8176, 'grad_norm': 2.2933754920959473, 'learning_rate': 8.283192940232652e-06, 'epoch': 2.5}\n",
      "{'loss': 1.8025, 'grad_norm': 2.3041763305664062, 'learning_rate': 7.882069795427196e-06, 'epoch': 2.53}\n",
      "{'loss': 1.8049, 'grad_norm': 2.338010549545288, 'learning_rate': 7.480946650621741e-06, 'epoch': 2.55}\n",
      "{'loss': 1.8002, 'grad_norm': 1.999851107597351, 'learning_rate': 7.079823505816285e-06, 'epoch': 2.58}\n",
      "{'loss': 1.8278, 'grad_norm': 2.322279214859009, 'learning_rate': 6.678700361010831e-06, 'epoch': 2.6}\n",
      "{'loss': 1.8112, 'grad_norm': 2.1688649654388428, 'learning_rate': 6.277577216205375e-06, 'epoch': 2.62}\n",
      "{'loss': 1.7974, 'grad_norm': 2.2594664096832275, 'learning_rate': 5.87645407139992e-06, 'epoch': 2.65}\n",
      "{'loss': 1.8216, 'grad_norm': 2.066502094268799, 'learning_rate': 5.475330926594465e-06, 'epoch': 2.67}\n",
      "{'loss': 1.7968, 'grad_norm': 2.33707594871521, 'learning_rate': 5.074207781789009e-06, 'epoch': 2.7}\n",
      "{'loss': 1.7997, 'grad_norm': 2.5030887126922607, 'learning_rate': 4.673084636983554e-06, 'epoch': 2.72}\n",
      "{'loss': 1.794, 'grad_norm': 2.3487486839294434, 'learning_rate': 4.271961492178099e-06, 'epoch': 2.74}\n",
      "{'loss': 1.7924, 'grad_norm': 2.2340965270996094, 'learning_rate': 3.870838347372643e-06, 'epoch': 2.77}\n",
      "{'loss': 1.8244, 'grad_norm': 2.150191307067871, 'learning_rate': 3.4697152025671884e-06, 'epoch': 2.79}\n",
      "{'loss': 1.7848, 'grad_norm': 2.281691789627075, 'learning_rate': 3.068592057761733e-06, 'epoch': 2.82}\n",
      "{'loss': 1.8137, 'grad_norm': 2.3715813159942627, 'learning_rate': 2.6674689129562775e-06, 'epoch': 2.84}\n",
      "{'loss': 1.8139, 'grad_norm': 2.3584306240081787, 'learning_rate': 2.2663457681508225e-06, 'epoch': 2.86}\n",
      "{'loss': 1.7941, 'grad_norm': 2.2120778560638428, 'learning_rate': 1.865222623345367e-06, 'epoch': 2.89}\n",
      "{'loss': 1.7965, 'grad_norm': 2.4630916118621826, 'learning_rate': 1.4640994785399118e-06, 'epoch': 2.91}\n",
      "{'loss': 1.8151, 'grad_norm': 2.1644060611724854, 'learning_rate': 1.0629763337344564e-06, 'epoch': 2.94}\n",
      "{'loss': 1.8045, 'grad_norm': 1.862902283668518, 'learning_rate': 6.618531889290013e-07, 'epoch': 2.96}\n",
      "{'loss': 1.8092, 'grad_norm': 2.163684129714966, 'learning_rate': 2.6073004412354593e-07, 'epoch': 2.98}\n",
      "{'train_runtime': 5498.9855, 'train_samples_per_second': 45.335, 'train_steps_per_second': 11.334, 'train_loss': 1.933819666746578, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-finetunedV2-3\\\\tokenizer_config.json',\n",
       " './gpt2-finetunedV2-3\\\\special_tokens_map.json',\n",
       " './gpt2-finetunedV2-3\\\\vocab.json',\n",
       " './gpt2-finetunedV2-3\\\\merges.txt',\n",
       " './gpt2-finetunedV2-3\\\\added_tokens.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        with open(file_path, \"r\") as f:\n",
    "            self.dialogs = f.read().split(\"\\n\\n\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dialogs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dialog = self.dialogs[idx]\n",
    "        encodings = self.tokenizer(dialog, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        return {key: torch.squeeze(val) for key, val in encodings.items()}\n",
    "    \n",
    "# Prepare your dataset\n",
    "train_dataset = DialogDataset(\n",
    "    file_path=\"processed_dialogs.txt\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetunedV2-3\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./gpt2-finetunedV2-3\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetunedV2-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<genre:drama><char:BIANCA><char:CAMERON>\n",
      "BIANCA: They're having a fight... again.\n",
      "CAMSON: I'm sorry.  I didn't mean to...\n",
      "BANCASSINO: You're not going to apologize for what happened. You know what I mean. I know you're sorry, but you can't apologize. It's not your fault. And you know that. So, I'll just say that I don't know why you did it. That I can understand. But I think you should apologize to me. Because I've been through this before. This is not the time to be apologizing. We're talking about a lifetime of pain. The pain of a child. A lifetime that's been lost. What's the point?\n",
      "CRAMS: It was a mistake. He was right. There was no mistake, Mom. No mistake at all. Just a bad mistake... and I\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetunedV2-3\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetunedV2-3\")\n",
    "\n",
    "# Prepare input\n",
    "input_text = \"<genre:drama><char:BIANCA><char:CAMERON>\\nBIANCA: They're having a fight... again.\"\n",
    "\n",
    "# Encode input\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate output\n",
    "output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decode and print the result\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
