{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TextDataset, DataCollatorForLanguageModeling\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aec552a5425a4437a82842ab97a1efd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15813 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4152, 'grad_norm': 3.140174388885498, 'learning_rate': 4.8419022323404794e-05, 'epoch': 0.03}\n",
      "{'loss': 2.3712, 'grad_norm': 2.992849826812744, 'learning_rate': 4.6838044646809586e-05, 'epoch': 0.06}\n",
      "{'loss': 2.3226, 'grad_norm': 2.2799367904663086, 'learning_rate': 4.5257066970214385e-05, 'epoch': 0.09}\n",
      "{'loss': 2.3184, 'grad_norm': 2.223583459854126, 'learning_rate': 4.367608929361918e-05, 'epoch': 0.13}\n",
      "{'loss': 2.266, 'grad_norm': 2.1571054458618164, 'learning_rate': 4.209511161702397e-05, 'epoch': 0.16}\n",
      "{'loss': 2.2801, 'grad_norm': 2.1511244773864746, 'learning_rate': 4.051413394042876e-05, 'epoch': 0.19}\n",
      "{'loss': 2.2761, 'grad_norm': 2.174072504043579, 'learning_rate': 3.893315626383356e-05, 'epoch': 0.22}\n",
      "{'loss': 2.2812, 'grad_norm': 1.7256395816802979, 'learning_rate': 3.735217858723835e-05, 'epoch': 0.25}\n",
      "{'loss': 2.2461, 'grad_norm': 2.0350048542022705, 'learning_rate': 3.5771200910643144e-05, 'epoch': 0.28}\n",
      "{'loss': 2.2678, 'grad_norm': 2.028801918029785, 'learning_rate': 3.4190223234047936e-05, 'epoch': 0.32}\n",
      "{'loss': 2.2384, 'grad_norm': 2.0904128551483154, 'learning_rate': 3.260924555745273e-05, 'epoch': 0.35}\n",
      "{'loss': 2.2367, 'grad_norm': 1.8867262601852417, 'learning_rate': 3.1028267880857526e-05, 'epoch': 0.38}\n",
      "{'loss': 2.2584, 'grad_norm': 2.0414886474609375, 'learning_rate': 2.9447290204262318e-05, 'epoch': 0.41}\n",
      "{'loss': 2.225, 'grad_norm': 1.911708950996399, 'learning_rate': 2.786631252766711e-05, 'epoch': 0.44}\n",
      "{'loss': 2.2283, 'grad_norm': 1.9679248332977295, 'learning_rate': 2.6285334851071902e-05, 'epoch': 0.47}\n",
      "{'loss': 2.2326, 'grad_norm': 1.8361082077026367, 'learning_rate': 2.4704357174476697e-05, 'epoch': 0.51}\n",
      "{'loss': 2.2506, 'grad_norm': 2.192887306213379, 'learning_rate': 2.3123379497881493e-05, 'epoch': 0.54}\n",
      "{'loss': 2.2126, 'grad_norm': 1.9174914360046387, 'learning_rate': 2.1542401821286285e-05, 'epoch': 0.57}\n",
      "{'loss': 2.203, 'grad_norm': 1.5193051099777222, 'learning_rate': 1.9961424144691077e-05, 'epoch': 0.6}\n",
      "{'loss': 2.218, 'grad_norm': 2.1238210201263428, 'learning_rate': 1.8380446468095872e-05, 'epoch': 0.63}\n",
      "{'loss': 2.1838, 'grad_norm': 1.9703052043914795, 'learning_rate': 1.6799468791500664e-05, 'epoch': 0.66}\n",
      "{'loss': 2.2175, 'grad_norm': 1.960647702217102, 'learning_rate': 1.521849111490546e-05, 'epoch': 0.7}\n",
      "{'loss': 2.1905, 'grad_norm': 2.0092661380767822, 'learning_rate': 1.3637513438310251e-05, 'epoch': 0.73}\n",
      "{'loss': 2.1843, 'grad_norm': 2.1512527465820312, 'learning_rate': 1.2056535761715045e-05, 'epoch': 0.76}\n",
      "{'loss': 2.1839, 'grad_norm': 1.8570852279663086, 'learning_rate': 1.0475558085119839e-05, 'epoch': 0.79}\n",
      "{'loss': 2.1915, 'grad_norm': 2.418527603149414, 'learning_rate': 8.894580408524632e-06, 'epoch': 0.82}\n",
      "{'loss': 2.1753, 'grad_norm': 1.9386308193206787, 'learning_rate': 7.313602731929425e-06, 'epoch': 0.85}\n",
      "{'loss': 2.1887, 'grad_norm': 2.206268310546875, 'learning_rate': 5.732625055334219e-06, 'epoch': 0.89}\n",
      "{'loss': 2.1758, 'grad_norm': 1.8272827863693237, 'learning_rate': 4.1516473787390124e-06, 'epoch': 0.92}\n",
      "{'loss': 2.216, 'grad_norm': 2.017228364944458, 'learning_rate': 2.5706697021438057e-06, 'epoch': 0.95}\n",
      "{'loss': 2.1991, 'grad_norm': 1.885785460472107, 'learning_rate': 9.896920255485993e-07, 'epoch': 0.98}\n",
      "{'train_runtime': 1354.2279, 'train_samples_per_second': 46.707, 'train_steps_per_second': 11.677, 'train_loss': 2.23960803610953, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-finetuned\\\\tokenizer_config.json',\n",
       " './gpt2-finetuned\\\\special_tokens_map.json',\n",
       " './gpt2-finetuned\\\\vocab.json',\n",
       " './gpt2-finetuned\\\\merges.txt',\n",
       " './gpt2-finetuned\\\\added_tokens.json')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Prepare your dataset\n",
    "train_dataset = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"processed_dialogs.txt\",\n",
    "    block_size=128)\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./gpt2-finetuned\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetuned\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<genre:comedy,romance><char:BIANCA><char:CAMERON>\n",
      "BIANCA: They're having a fight... again.\n",
      "CAMPBELL: I'm sorry, I didn't mean to...\n",
      "BANDA: You're not going to be able to get out of here.\n",
      "\n",
      "\n",
      "<char:\"BIanCA\"><char\":CEMETER>\n",
      "\n",
      "<family:biography,drama,history><family:\"BENNY><b:BUD><d:DOUG>BUNNY: What's the matter?\n",
      "DUDG: Nothing.  I just want to talk to you. I want you to know that I love you, and I don't want anyone to see that. You know, you're a good man. And I know you love me. But I can't let you go. It's not right. We're going back to the hotel. The hotel is closed\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetuned\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetuned\")\n",
    "\n",
    "# Prepare input\n",
    "input_text = \"<genre:comedy,romance><char:BIANCA><char:CAMERON>\\nBIANCA: They're having a fight... again.\"\n",
    "\n",
    "# Encode input\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate output\n",
    "output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decode and print the result\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cf16b4105d34026848784615dc262f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20775 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3665, 'grad_norm': 4.240726470947266, 'learning_rate': 4.879663056558363e-05, 'epoch': 0.02}\n",
      "{'loss': 2.2709, 'grad_norm': 2.9654688835144043, 'learning_rate': 4.759326113116727e-05, 'epoch': 0.05}\n",
      "{'loss': 2.2172, 'grad_norm': 3.0672502517700195, 'learning_rate': 4.63898916967509e-05, 'epoch': 0.07}\n",
      "{'loss': 2.2186, 'grad_norm': 3.00846004486084, 'learning_rate': 4.518652226233454e-05, 'epoch': 0.1}\n",
      "{'loss': 2.2369, 'grad_norm': 3.286755323410034, 'learning_rate': 4.398315282791817e-05, 'epoch': 0.12}\n",
      "{'loss': 2.2261, 'grad_norm': 2.3606505393981934, 'learning_rate': 4.277978339350181e-05, 'epoch': 0.14}\n",
      "{'loss': 2.1868, 'grad_norm': 3.0841352939605713, 'learning_rate': 4.1576413959085445e-05, 'epoch': 0.17}\n",
      "{'loss': 2.1673, 'grad_norm': 2.4785640239715576, 'learning_rate': 4.0373044524669075e-05, 'epoch': 0.19}\n",
      "{'loss': 2.1476, 'grad_norm': 2.0501327514648438, 'learning_rate': 3.916967509025271e-05, 'epoch': 0.22}\n",
      "{'loss': 2.1452, 'grad_norm': 2.8682167530059814, 'learning_rate': 3.7966305655836343e-05, 'epoch': 0.24}\n",
      "{'loss': 2.1499, 'grad_norm': 2.4254660606384277, 'learning_rate': 3.676293622141998e-05, 'epoch': 0.26}\n",
      "{'loss': 2.1325, 'grad_norm': 2.516324758529663, 'learning_rate': 3.555956678700361e-05, 'epoch': 0.29}\n",
      "{'loss': 2.1318, 'grad_norm': 2.9864113330841064, 'learning_rate': 3.435619735258724e-05, 'epoch': 0.31}\n",
      "{'loss': 2.106, 'grad_norm': 2.7039968967437744, 'learning_rate': 3.315282791817088e-05, 'epoch': 0.34}\n",
      "{'loss': 2.1483, 'grad_norm': 2.3638644218444824, 'learning_rate': 3.194945848375451e-05, 'epoch': 0.36}\n",
      "{'loss': 2.0995, 'grad_norm': 2.334022283554077, 'learning_rate': 3.074608904933815e-05, 'epoch': 0.39}\n",
      "{'loss': 2.089, 'grad_norm': 2.675896406173706, 'learning_rate': 2.9542719614921782e-05, 'epoch': 0.41}\n",
      "{'loss': 2.1032, 'grad_norm': 2.544827938079834, 'learning_rate': 2.8339350180505413e-05, 'epoch': 0.43}\n",
      "{'loss': 2.0655, 'grad_norm': 2.284426689147949, 'learning_rate': 2.713598074608905e-05, 'epoch': 0.46}\n",
      "{'loss': 2.07, 'grad_norm': 2.134601593017578, 'learning_rate': 2.5932611311672685e-05, 'epoch': 0.48}\n",
      "{'loss': 2.0614, 'grad_norm': 2.6997532844543457, 'learning_rate': 2.472924187725632e-05, 'epoch': 0.51}\n",
      "{'loss': 2.0753, 'grad_norm': 2.2453227043151855, 'learning_rate': 2.3525872442839953e-05, 'epoch': 0.53}\n",
      "{'loss': 2.0742, 'grad_norm': 2.2066268920898438, 'learning_rate': 2.2322503008423587e-05, 'epoch': 0.55}\n",
      "{'loss': 2.0416, 'grad_norm': 1.9847118854522705, 'learning_rate': 2.111913357400722e-05, 'epoch': 0.58}\n",
      "{'loss': 2.0586, 'grad_norm': 2.6713151931762695, 'learning_rate': 1.9915764139590855e-05, 'epoch': 0.6}\n",
      "{'loss': 2.0531, 'grad_norm': 2.2925753593444824, 'learning_rate': 1.871239470517449e-05, 'epoch': 0.63}\n",
      "{'loss': 2.0319, 'grad_norm': 1.865940809249878, 'learning_rate': 1.7509025270758123e-05, 'epoch': 0.65}\n",
      "{'loss': 2.0443, 'grad_norm': 2.0766971111297607, 'learning_rate': 1.6305655836341757e-05, 'epoch': 0.67}\n",
      "{'loss': 2.0162, 'grad_norm': 3.1454668045043945, 'learning_rate': 1.5102286401925391e-05, 'epoch': 0.7}\n",
      "{'loss': 2.045, 'grad_norm': 2.6898529529571533, 'learning_rate': 1.3898916967509026e-05, 'epoch': 0.72}\n",
      "{'loss': 2.0445, 'grad_norm': 2.086174726486206, 'learning_rate': 1.2695547533092661e-05, 'epoch': 0.75}\n",
      "{'loss': 2.0194, 'grad_norm': 2.18049955368042, 'learning_rate': 1.1492178098676294e-05, 'epoch': 0.77}\n",
      "{'loss': 2.0324, 'grad_norm': 2.4252371788024902, 'learning_rate': 1.028880866425993e-05, 'epoch': 0.79}\n",
      "{'loss': 2.0323, 'grad_norm': 2.341942071914673, 'learning_rate': 9.085439229843562e-06, 'epoch': 0.82}\n",
      "{'loss': 2.0442, 'grad_norm': 2.3043715953826904, 'learning_rate': 7.882069795427196e-06, 'epoch': 0.84}\n",
      "{'loss': 2.0448, 'grad_norm': 2.684467315673828, 'learning_rate': 6.678700361010831e-06, 'epoch': 0.87}\n",
      "{'loss': 2.0302, 'grad_norm': 2.314789056777954, 'learning_rate': 5.475330926594465e-06, 'epoch': 0.89}\n",
      "{'loss': 2.0343, 'grad_norm': 2.1284968852996826, 'learning_rate': 4.271961492178099e-06, 'epoch': 0.91}\n",
      "{'loss': 2.0293, 'grad_norm': 2.4306623935699463, 'learning_rate': 3.068592057761733e-06, 'epoch': 0.94}\n",
      "{'loss': 2.0188, 'grad_norm': 2.174391508102417, 'learning_rate': 1.865222623345367e-06, 'epoch': 0.96}\n",
      "{'loss': 2.0102, 'grad_norm': 2.68711519241333, 'learning_rate': 6.618531889290013e-07, 'epoch': 0.99}\n",
      "{'train_runtime': 1919.5813, 'train_samples_per_second': 43.29, 'train_steps_per_second': 10.823, 'train_loss': 2.0993883645692315, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./gpt2-finetunedV2\\\\tokenizer_config.json',\n",
       " './gpt2-finetunedV2\\\\special_tokens_map.json',\n",
       " './gpt2-finetunedV2\\\\vocab.json',\n",
       " './gpt2-finetunedV2\\\\merges.txt',\n",
       " './gpt2-finetunedV2\\\\added_tokens.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        with open(file_path, \"r\") as f:\n",
    "            self.dialogs = f.read().split(\"\\n\\n\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dialogs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dialog = self.dialogs[idx]\n",
    "        encodings = self.tokenizer(dialog, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        return {key: torch.squeeze(val) for key, val in encodings.items()}\n",
    "    \n",
    "# Prepare your dataset\n",
    "train_dataset = DialogDataset(\n",
    "    file_path=\"processed_dialogs.txt\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetunedV2\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./gpt2-finetunedV2\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetunedV2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<genre:comedy,romance><char:BIANCA><char:CAMERON>\n",
      "BIANCA: They're having a fight... again.\n",
      "CAMS: I'm sorry, I don't know.  I just thought I'd ask you something. I mean, you're a very nice guy, but you donï¿½t know how to handle it. Youïve got a lot of problems, and youïll have to deal with them. But Iïm not going to let you get away with it, do you?\n",
      "BANCHO: No, no, not at all. Itïs just that I have a little problem with the way you treat me. And I think you should go to the police. Theyïre going after you. If you do, they're going back to jail. Thatïd be a good thing. The police are going out to get you, too. So I'll just go\n"
     ]
    }
   ],
   "source": [
    "# Load fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetunedV2\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetunedV2\")\n",
    "\n",
    "# Prepare input\n",
    "input_text = \"<genre:comedy,romance><char:BIANCA><char:CAMERON>\\nBIANCA: They're having a fight... again.\"\n",
    "\n",
    "# Encode input\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate output\n",
    "output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decode and print the result\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model V2 - 3 Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86724df98f384e809c50866c5997fc95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/62325 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.3664, 'grad_norm': 4.189531326293945, 'learning_rate': 4.959887685519455e-05, 'epoch': 0.02}\n",
      "{'loss': 2.2713, 'grad_norm': 2.9324004650115967, 'learning_rate': 4.9197753710389094e-05, 'epoch': 0.05}\n",
      "{'loss': 2.2188, 'grad_norm': 3.052821159362793, 'learning_rate': 4.879663056558363e-05, 'epoch': 0.07}\n",
      "{'loss': 2.2199, 'grad_norm': 2.948228597640991, 'learning_rate': 4.839550742077818e-05, 'epoch': 0.1}\n",
      "{'loss': 2.2387, 'grad_norm': 3.591900110244751, 'learning_rate': 4.7994384275972725e-05, 'epoch': 0.12}\n",
      "{'loss': 2.2278, 'grad_norm': 2.323040246963501, 'learning_rate': 4.759326113116727e-05, 'epoch': 0.14}\n",
      "{'loss': 2.1895, 'grad_norm': 3.0196328163146973, 'learning_rate': 4.7192137986361816e-05, 'epoch': 0.17}\n",
      "{'loss': 2.1706, 'grad_norm': 2.442878484725952, 'learning_rate': 4.6791014841556355e-05, 'epoch': 0.19}\n",
      "{'loss': 2.1507, 'grad_norm': 1.990504264831543, 'learning_rate': 4.63898916967509e-05, 'epoch': 0.22}\n",
      "{'loss': 2.1482, 'grad_norm': 2.855936050415039, 'learning_rate': 4.598876855194545e-05, 'epoch': 0.24}\n",
      "{'loss': 2.1528, 'grad_norm': 2.2785706520080566, 'learning_rate': 4.558764540713999e-05, 'epoch': 0.26}\n",
      "{'loss': 2.1351, 'grad_norm': 2.4638025760650635, 'learning_rate': 4.518652226233454e-05, 'epoch': 0.29}\n",
      "{'loss': 2.1343, 'grad_norm': 2.853442668914795, 'learning_rate': 4.478539911752908e-05, 'epoch': 0.31}\n",
      "{'loss': 2.1083, 'grad_norm': 2.5983026027679443, 'learning_rate': 4.4384275972723624e-05, 'epoch': 0.34}\n",
      "{'loss': 2.1506, 'grad_norm': 2.2574121952056885, 'learning_rate': 4.398315282791817e-05, 'epoch': 0.36}\n",
      "{'loss': 2.1015, 'grad_norm': 2.2202956676483154, 'learning_rate': 4.3582029683112715e-05, 'epoch': 0.39}\n",
      "{'loss': 2.0908, 'grad_norm': 2.538898468017578, 'learning_rate': 4.318090653830726e-05, 'epoch': 0.41}\n",
      "{'loss': 2.1049, 'grad_norm': 2.396144390106201, 'learning_rate': 4.277978339350181e-05, 'epoch': 0.43}\n",
      "{'loss': 2.0668, 'grad_norm': 2.1366138458251953, 'learning_rate': 4.237866024869635e-05, 'epoch': 0.46}\n",
      "{'loss': 2.0715, 'grad_norm': 1.9840930700302124, 'learning_rate': 4.19775371038909e-05, 'epoch': 0.48}\n",
      "{'loss': 2.0626, 'grad_norm': 2.468315362930298, 'learning_rate': 4.1576413959085445e-05, 'epoch': 0.51}\n",
      "{'loss': 2.075, 'grad_norm': 2.043100595474243, 'learning_rate': 4.117529081427999e-05, 'epoch': 0.53}\n",
      "{'loss': 2.0746, 'grad_norm': 2.0409326553344727, 'learning_rate': 4.0774167669474536e-05, 'epoch': 0.55}\n",
      "{'loss': 2.0415, 'grad_norm': 1.8235228061676025, 'learning_rate': 4.0373044524669075e-05, 'epoch': 0.58}\n",
      "{'loss': 2.058, 'grad_norm': 2.488405704498291, 'learning_rate': 3.997192137986362e-05, 'epoch': 0.6}\n",
      "{'loss': 2.0523, 'grad_norm': 2.132418394088745, 'learning_rate': 3.957079823505817e-05, 'epoch': 0.63}\n",
      "{'loss': 2.0295, 'grad_norm': 1.726885199546814, 'learning_rate': 3.916967509025271e-05, 'epoch': 0.65}\n",
      "{'loss': 2.0428, 'grad_norm': 1.9037384986877441, 'learning_rate': 3.876855194544726e-05, 'epoch': 0.67}\n",
      "{'loss': 2.014, 'grad_norm': 2.738478899002075, 'learning_rate': 3.83674288006418e-05, 'epoch': 0.7}\n",
      "{'loss': 2.043, 'grad_norm': 2.4652419090270996, 'learning_rate': 3.7966305655836343e-05, 'epoch': 0.72}\n",
      "{'loss': 2.0418, 'grad_norm': 1.863530158996582, 'learning_rate': 3.756518251103089e-05, 'epoch': 0.75}\n",
      "{'loss': 2.0151, 'grad_norm': 1.8262279033660889, 'learning_rate': 3.7164059366225435e-05, 'epoch': 0.77}\n",
      "{'loss': 2.0283, 'grad_norm': 2.1691691875457764, 'learning_rate': 3.676293622141998e-05, 'epoch': 0.79}\n",
      "{'loss': 2.0274, 'grad_norm': 2.0588746070861816, 'learning_rate': 3.636181307661452e-05, 'epoch': 0.82}\n",
      "{'loss': 2.0386, 'grad_norm': 1.9992061853408813, 'learning_rate': 3.5960689931809066e-05, 'epoch': 0.84}\n",
      "{'loss': 2.0378, 'grad_norm': 2.9637575149536133, 'learning_rate': 3.555956678700361e-05, 'epoch': 0.87}\n",
      "{'loss': 2.0213, 'grad_norm': 2.042381763458252, 'learning_rate': 3.515844364219816e-05, 'epoch': 0.89}\n",
      "{'loss': 2.0258, 'grad_norm': 1.9771448373794556, 'learning_rate': 3.47573204973927e-05, 'epoch': 0.91}\n",
      "{'loss': 2.0175, 'grad_norm': 2.219280958175659, 'learning_rate': 3.435619735258724e-05, 'epoch': 0.94}\n",
      "{'loss': 2.0065, 'grad_norm': 2.125303030014038, 'learning_rate': 3.395507420778179e-05, 'epoch': 0.96}\n",
      "{'loss': 1.9959, 'grad_norm': 2.3337771892547607, 'learning_rate': 3.3553951062976334e-05, 'epoch': 0.99}\n",
      "{'loss': 1.9757, 'grad_norm': 2.3443100452423096, 'learning_rate': 3.315282791817088e-05, 'epoch': 1.01}\n",
      "{'loss': 1.9236, 'grad_norm': 1.7995961904525757, 'learning_rate': 3.2751704773365426e-05, 'epoch': 1.03}\n",
      "{'loss': 1.9174, 'grad_norm': 1.884700894355774, 'learning_rate': 3.2350581628559965e-05, 'epoch': 1.06}\n",
      "{'loss': 1.8941, 'grad_norm': 2.372041940689087, 'learning_rate': 3.194945848375451e-05, 'epoch': 1.08}\n",
      "{'loss': 1.9119, 'grad_norm': 2.355761766433716, 'learning_rate': 3.1548335338949056e-05, 'epoch': 1.11}\n",
      "{'loss': 1.9012, 'grad_norm': 2.0313162803649902, 'learning_rate': 3.11472121941436e-05, 'epoch': 1.13}\n",
      "{'loss': 1.9045, 'grad_norm': 2.390505790710449, 'learning_rate': 3.074608904933815e-05, 'epoch': 1.16}\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer, max_length):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        with open(file_path, \"r\") as f:\n",
    "            self.dialogs = f.read().split(\"\\n\\n\")\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.dialogs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dialog = self.dialogs[idx]\n",
    "        encodings = self.tokenizer(dialog, truncation=True, padding='max_length', max_length=self.max_length, return_tensors='pt')\n",
    "        return {key: torch.squeeze(val) for key, val in encodings.items()}\n",
    "    \n",
    "# Prepare your dataset\n",
    "train_dataset = DialogDataset(\n",
    "    file_path=\"processed_dialogs.txt\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=128\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetunedV2-3\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "model.save_pretrained(\"./gpt2-finetunedV2-3\")\n",
    "tokenizer.save_pretrained(\"./gpt2-finetunedV2-3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained(\"./gpt2-finetunedV2-3\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"./gpt2-finetunedV2-3\")\n",
    "\n",
    "# Prepare input\n",
    "input_text = \"<genre:comedy,romance><char:BIANCA><char:CAMERON>\\nBIANCA: They're having a fight... again.\"\n",
    "\n",
    "# Encode input\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# Generate output\n",
    "output = model.generate(input_ids, max_length=200, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "\n",
    "# Decode and print the result\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
